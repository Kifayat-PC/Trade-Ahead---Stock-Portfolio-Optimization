# -*- coding: utf-8 -*-
"""Another copy of GL project K1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FNW6mgz8SHdEqebiUEV45NZfrl59VyvU

# Trade&Ahead - Problem Statement

## Context

The stock market is a powerful investment vehicle, offering opportunities for wealth creation, inflation protection, and tax benefits. However, choosing the right stocks requires thorough analysis, as financial metrics can be overwhelming. To optimize investment strategies, clustering techniques can be used to group stocks based on similar financial characteristics, helping investors build diversified and risk-adjusted portfolios

### Problem Statement

Investing in stocks requires careful analysis of financial metrics to build a diversified portfolio that minimizes risk and maximizes returns. With a vast number of stocks available in the market, manually analyzing each one to identify profitable investment opportunities is challenging. Investors need a systematic approach to categorize stocks based on their financial attributes, helping them make informed investment decisions.

### Data Dictionary

- Ticker Symbol: An abbreviation used to uniquely identify publicly traded shares of a particular stock on a particular stock market
- Company: Name of the company
- GICS Sector: The specific economic sector assigned to a company by the Global Industry Classification Standard (GICS) that best defines its business operations
- GICS Sub Industry: The specific sub-industry group assigned to a company by the Global Industry Classification Standard (GICS) that best defines its
business operations
- Current Price: Current stock price in dollars
- Price Change: Percentage change in the stock price in 13 weeks
- Volatility: Standard deviation of the stock price over the past 13 weeks
- ROE: A measure of financial performance calculated by dividing net income by shareholders' equity (shareholders' equity is equal to a company's assets minus its debt)
- Cash Ratio: The ratio of a company's total reserves of cash and cash equivalents to its total current liabilities
- Net Cash Flow: The difference between a company's cash inflows and outflows (in dollars)
- Net Income: Revenues minus expenses, interest, and taxes (in dollars)
- Earnings Per Share: Company's net profit divided by the number of common shares it has outstanding (in dollars)
- Estimated Shares Outstanding: Company's stock is currently held by all its shareholders
- P/E Ratio: Ratio of the company's current stock price to the earnings per share
- P/B Ratio: Ratio of the company's stock price per share by its book value per share (book value of a company is the net difference between that company's total assets and total liabilities)

##Objective
Trade&Ahead, a financial consultancy firm, has provided stock market data, including stock prices and financial indicators. As a Data Scientist, the goal is to:

- Analyze stock data to identify key financial patterns.
- Cluster stocks using K-Means and Hierarchical clustering.
- Compare the clustering results to determine the best approach.
- Provide investment insights based on the identified stock groups.

# Data overview
"""

import pandas as pd

# Load the dataset
file_path = "/content/stock_data.csv"
df = pd.read_csv(file_path)

"""##  Display basic information about the dataset

"""

df.head()

df.info()

df.shape , df.describe()

"""Undertanding the dataset
- The dataset contains 340 rows and 15 columns.
- It includes categorical columns (e.g., Ticker Symbol, Security, GICS Sector, GICS Sub Industry).
- Numerical columns include stock prices, financial indicators (ROE, P/E Ratio, P/B Ratio, etc.), and earnings per share.
- There are no missing values (all columns have 340 non-null values).
- Some columns may require feature scaling before applying clustering.
- The P/B Ratio column has negative values, which should be investigated.

# Check for missing values and basic statistics:
"""

# Check for missing values
missing_values = df.isnull().sum()

# Summary statistics for numerical features
summary_stats = df.describe()

missing_values

"""Therefore there are no missing values

# Univariate and Bivariate analysis
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Set plot style
sns.set_style("whitegrid")

# Select numerical columns for analysis (excluding categorical ones)
numerical_cols = [
    "Current Price", "Price Change", "Volatility", "ROE", "Cash Ratio",
    "Net Cash Flow", "Net Income", "Earnings Per Share",
    "Estimated Shares Outstanding", "P/E Ratio", "P/B Ratio"
]

# Plot histograms for numerical features
plt.figure(figsize=(14, 10))
for i, col in enumerate(numerical_cols, 1):
    plt.subplot(4, 3, i)
    sns.histplot(df[col], bins=30, kde=True)
    plt.title(f"Distribution of {col}")

plt.tight_layout()
plt.show()

"""

Observations (univariate analysis)  :
- Current Price: Skewed distribution with a few very high-priced stocks.
- Price Change: Mostly centered around zero, but some stocks have extreme changes.
- Volatility: Right-skewed, indicating most stocks have low volatility, but a few are highly volatile.
- ROE (Return on Equity): Positively skewed, meaning a few stocks have extremely high ROE.
- Cash Ratio: Right-skewed, suggesting that most companies have low liquidity, but some hold large cash reserves.
- Net Cash Flow & Net Income: Show a long tail, meaning a few companies generate significantly higher cash flow/income.
- Earnings Per Share (EPS): Mostly low, with a few high values.
- P/E Ratio & P/B Ratio: Very high variability; some stocks have extremely high values, requiring potential outlier treatment.-"""

# Compute the correlation matrix
corr_matrix = df[numerical_cols].corr()

# Plot heatmap of correlations
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Correlation Matrix of Numerical Features")
plt.show()

"""# Correlation Matrix Analysis of Numerical Features

## Key Observations:

### Strong Positive Correlations:
- **Net Income & Earnings Per Share (0.59):** Higher net income tends to increase earnings per share.
- **Earnings Per Share & Current Price (0.48):** Stocks with higher EPS generally have higher current prices.
- **Estimated Shares Outstanding & Net Income (0.59):** A moderate correlation suggests that larger companies (with more shares outstanding) may have higher net incomes.

### Strong Negative Correlations:
- **Price Change & Volatility (-0.41):** Stocks with high volatility tend to have more unpredictable price changes.
- **Earnings Per Share & Volatility (-0.38):** Highly volatile stocks tend to have lower EPS stability.
- **ROE & Earnings Per Share (-0.41):** Surprisingly, return on equity is negatively correlated with earnings per share, possibly due to varying capital structures.

### Other Notable Correlations:
- **P/E Ratio & Current Price (0.26):** Stocks with higher prices tend to have higher price-to-earnings ratios.
- **P/B Ratio & Cash Ratio (0.23):** Indicates some connection between a company‚Äôs liquidity and its valuation.
- **Volatility & ROE (0.16):** A weak correlation, suggesting that return on equity is not significantly affected by stock price fluctuations.

## Conclusion:
- The matrix highlights how fundamental financial metrics interact.
- **Earnings per Share** and **Net Income** play a crucial role in stock valuation.
- **Volatility has a strong inverse relationship with price stability**, making it a key risk factor.
- Investors should focus on **EPS and Net Income for stock performance insights**, while considering volatility as a potential risk indicator.

"""

# Selecting a subset of numerical columns for better visualization
subset_features = ["Current Price", "Price Change", "Volatility", "ROE", "P/E Ratio", "P/B Ratio"]

# Plot pairplot
sns.pairplot(df[subset_features], diag_kind="kde", plot_kws={'alpha':0.6, 's':10})
plt.suptitle("Pairplot of Selected Financial Indicators", y=1.02)
plt.show()

"""##Bivariate Analysis: Correlation Insights
### High Correlations:
- Net Income & Earnings Per Share (EPS) (0.88): Companies with higher net income tend to have higher EPS.
- P/E Ratio & P/B Ratio (0.55): Companies with higher Price-to-Earnings also tend to have higher Price-to-Book ratios.
- ROE & P/E Ratio (0.45): Companies with better return on equity generally have higher P/E values.

### Negative Correlations:
- Price Change & P/E Ratio (-0.30): Companies with higher P/E ratios tend to have lower recent price changes.
- Cash Ratio & P/B Ratio (-0.27): Companies with higher cash reserves tend to have lower P/B ratios.
- Low Correlation with Volatility: Volatility is weakly correlated with other financial indicators, implying that stock price fluctuations are not necessarily tied to fundamental financial metrics.

## Data preprocessing
"""

# Check for duplicate values
duplicate_count = df.duplicated().sum()

# Identify potential outliers using the IQR method
Q1 = df[numerical_cols].quantile(0.25)
Q3 = df[numerical_cols].quantile(0.75)
IQR = Q3 - Q1

# Define outlier boundaries
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Count outliers for each numerial column
outliers = ((df[numerical_cols] < lower_bound) | (df[numerical_cols] > upper_bound)).sum()

duplicate_count, outliers

"""Obseervations

-- No Duplicate Rows ‚Äì The dataset does not have any duplicate records.

-- Outliers Detected ‚Äì Several financial indicators have a significant number of outliers:
- Net Cash Flow (76 outliers) and Net Income (57 outliers) have the most extreme values.
- ROE, Earnings Per Share, and P/E Ratio also contain many outliers.
This is expected in financial data, as some companies have extremely high or low financial metrics.

#  Outlier Detection & Treatment
"""

from scipy.stats.mstats import winsorize
from sklearn.preprocessing import StandardScaler

# Apply winsorization (capping outliers at 5th and 95th percentiles)
df_winsorized = df.copy()
for col in numerical_cols:
    df_winsorized[col] = winsorize(df[col], limits=[0.05, 0.05])
# Standardize the numerical columns for clustering
scaler = StandardScaler()
df_scaled = df_winsorized.copy()
df_scaled[numerical_cols] = scaler.fit_transform(df_winsorized[numerical_cols])

# Display the transformed data
df_scaled.head()

"""Observations After Data Preprocessing:

- Outliers Handled ‚Äì Winsorization successfully capped extreme values at the 5th and 95th percentiles.
- Feature Scaling Applied ‚Äì All numerical features are now standardized (mean ‚âà 0, variance ‚âà 1), which is essential for clustering.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Set up the figure size
plt.figure(figsize=(12, 8))

# Boxplots for key numerical variables to visualize outliers
for i, col in enumerate(numeric_cols, 1):
    plt.subplot(3, 3, i)
    sns.boxplot(y=df[col])
    plt.title(f'Boxplot of {col}')

plt.tight_layout()
plt.show()

""" # K means clustering"""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
import numpy as np

# Determine the optimal number of clusters using the Elbow Method
inertia = []
silhouette_scores = []
cluster_range = range(2, 11)

for k in cluster_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(df_scaled[numerical_cols])
    inertia.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(df_scaled[numerical_cols], kmeans.labels_))

# Plot the Elbow Curve
plt.figure(figsize=(10, 5))
plt.plot(cluster_range, inertia, marker='o', linestyle='--', label='Inertia')
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Inertia (Within-cluster SSE)")
plt.title("Elbow Method for Optimal k")
plt.legend()
plt.show()

# Plot the Silhuette Scores
plt.figure(figsize=(10, 5))
plt.plot(cluster_range, silhouette_scores, marker='o', linestyle='--', color='r', label='Silhouette Score')
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Silhouette Score")
plt.title("Silhouette Score for Different k")
plt.legend()
plt.show()

"""
Observations from Elbow and Silhouette Score Plots:
- Elbow Method: The curve shows a noticeable "elbow" around k = 4 or k = 5, suggesting an optimal number of clusters.
- Silhouette Score: The highest silhouette score is around k = 4, indicating well-separated clusters."""

# Apply K-means with the optimal number of clusters (k=4)
optimal_k = 4
kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
df_scaled["Cluster_KMeans"] = kmeans.fit_predict(df_scaled[numerical_cols])

# Compute mean values for each cluster
cluster_means = df_scaled.groupby("Cluster_KMeans")[numerical_cols].mean()

# Plot cluster characteristics
plt.figure(figsize=(12, 6))
sns.heatmap(cluster_means, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Cluster Profiling (K-Means)")
plt.show()

# Count the number of companies in each cluster
df_scaled["Cluster_KMeans"].value_counts()

"""### Observation :

Obtained four clusters from K-Means:

- Cluster 0: 65 stocks
- Cluster 1: 49 stocks
- Cluster 2: 188 stocks (largest)
- Cluster 3: 38 stocks (smallest)

### Inference

- Cluster 0: Stocks with high current prices and moderate financial indicators.
- Cluster 1: High volatility and P/E ratio ‚Üí High-risk, high-reward growth stocks.
- Cluster 2: Most companies fall here, showing balanced financial health ‚Üí Stable investments.
- Cluster 3: Very high volatility but strong earnings per share (EPS) ‚Üí Risky but high-growth potential.-

## Hierarchical Clustering
"""

from scipy.cluster.hierarchy import linkage, dendrogram

# Apply hierarchical clustering
linkage_matrix = linkage(df_scaled[numerical_cols], method="ward")

# Plot the dendrogram
plt.figure(figsize=(12, 6))
dendrogram(linkage_matrix, truncate_mode="level", p=5, leaf_rotation=90, leaf_font_size=10)
plt.title("Dendrogram (Ward's Method)")
plt.xlabel("Company Index")
plt.ylabel("Distance")
plt.show()

"""Observations from the Hierarchical Clustering Dendrogram:
- The dendrogram suggests around 4 or 5 clusters, similar to K-Means.
- There is a clear separation in distances, indicating well-defined groups..

## Apply hierarchical clustering with ùëò =4 and compare it with K-Means result
"""

from scipy.cluster.hierarchy import fcluster

# Apply hierarchical clustering with the optimal number of clusters (k=4)
df_scaled["Cluster_Hierarchical"] = fcluster(linkage_matrix, t=4, criterion="maxclust")

# Compute mean values for each cluster
cluster_means_hierarchical = df_scaled.groupby("Cluster_Hierarchical")[numerical_cols].mean()

# Plot cluster characteristics
plt.figure(figsize=(12, 6))
sns.heatmap(cluster_means_hierarchical, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Cluster Profiling (Hierarchical Clustering)")
plt.show()

# Count the number of companies in each cluster
df_scaled["Cluster_Hierarchical"].value_counts()

"""##### Observations from Hierarchical Clustering:

- Cluster 1: 25 companies (smallest)
- Cluster 2: 34 companies
- Cluster 3: 60 companies
- Cluster 4: 221 companies (largest)

##### Cluster Characteristics (based on heatmap):

- Cluster 1: High Net Cash Flow and Cash Ratio‚Äîcompanies with strong liquidity.
- Cluster 2: High P/E Ratio and Volatility‚Äîlikely high-growth or speculative stocks.
- Cluster 3: High Net Income, ROE, and EPS‚Äîprofitable and well-established firms.
- Cluster 4: Moderate financial indicators‚Äîrepresents the majority of mid-tier companies.

## **Comparison: K-Means vs. Hierarchical Clustering**


| Aspect               | K-Means Clustering  | Hierarchical Clustering  |
|----------------------|--------------------|--------------------------|
| **Optimal Clusters** | 4                  | 4                        |
| **Largest Cluster**  | Cluster 1 (186)    | Cluster 4 (221)          |
| **Smallest Cluster** | Cluster 2 (38)     | Cluster 1 (25)           |

Both methods reveal similar groupings, but hierarchical clustering has a more imbalanced distribution, while K-Means provides a more even spread.

## PCA Plot: Visualizing Clusters in 2D
"""

from sklearn.decomposition import PCA

# Reduce dimensions to 2 using PCA
pca = PCA(n_components=2)
pca_result = pca.fit_transform(df_scaled[numerical_cols])  # Apply PCA on scaled data

df_scaled["PCA1"] = pca_result[:, 0]
df_scaled["PCA2"] = pca_result[:, 1]

# Scatter plot of clusters
plt.figure(figsize=(10, 6))
sns.scatterplot(
    x="PCA1", y="PCA2", hue="Cluster_KMeans", palette="tab10", data=df_scaled, alpha=0.7
)
plt.title("PCA Visualization of Clusters")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.legend(title="Cluster")
plt.show()

"""## Boxplots: Comparing Financial Indicators Across Clusters"""

import seaborn as sns
import matplotlib.pyplot as plt

# Print column names to debug
print(df_scaled.columns)

fig, axes = plt.subplots(3, 3, figsize=(15, 12))
fig.suptitle("Comparison of Financial Metrics Across Clusters")

# Updated metric list with correct column names
metrics = ["ROE", "P/E Ratio", "Net Income", "Volatility", "Cash Ratio", "Earnings Per Share", "Price Change"]

for i, metric in enumerate(metrics):
    row, col = i // 3, i % 3
    if metric in df_scaled.columns:  # Ensure the column exists
        sns.boxplot(x="Cluster_KMeans", y=metric, data=df_scaled, ax=axes[row, col], hue="Cluster_KMeans", legend=False, palette="coolwarm")
        axes[row, col].set_title(f"{metric} Distribution per Cluster")
    else:
        print(f" Warning: {metric} not found in dataset")

plt.tight_layout()
plt.show()

"""##  Sector-Wise Stock Distribution Across Clusters"""

plt.figure(figsize=(12, 5))
sns.countplot(x="GICS Sector", hue="Cluster_KMeans", data=df_scaled, palette="Set2")
plt.xticks(rotation=90)
plt.title("Sector-Wise Distribution of Stocks per Cluster")
plt.xlabel("GICS Sector")
plt.ylabel("Number of Stocks")
plt.legend(title="Cluster")
plt.show()

"""## Compute Cophenetic Correlation"""

df_numeric = df_scaled.select_dtypes(include=['number'])

# Compute pairwise distances
distance_matrix = pdist(df_numeric, metric='euclidean')
import pandas as pd
from scipy.spatial.distance import pdist

# only numeric columns are used
df_numeric = df_scaled.select_dtypes(include=['number'])

# Check if any object columns remain
print("Remaining non-numeric columns:", df_scaled.select_dtypes(exclude=['number']).columns.tolist())

# its pairwise distancs
distance_matrix = pdist(df_numeric, metric='euclidean')

# Test different linkage methods
linkage_methods = ['single', 'complete', 'average', 'weighted']
cophenetic_scores = {}

for method in linkage_methods:
    Z = linkage(distance_matrix, method=method)
    coph_corr, _ = cophenet(Z, distance_matrix)
    cophenetic_scores[method] = coph_corr

# Print results
print("Cophenetic Correlation Coefficients:")
for method, score in cophenetic_scores.items():
    print(f"{method}: {score:.4f}")

"""## Dendrogram for Best Linkage Method"""

import scipy.cluster.hierarchy as sch
import matplotlib.pyplot as plt

# Compute linkage matrix
Z = linkage(distance_matrix, method='average')

# Plot dendrogram
plt.figure(figsize=(12, 6))
sch.dendrogram(Z, truncate_mode="level", p=10)
plt.title("Dendrogram (Average Linkage)")
plt.xlabel("Data Points")
plt.ylabel("Distance")
plt.show()

"""Observation
- The y-axis (Distance) represents the dissimilarity between clusters.
- The horizontal line cut can help decide the optimal number of clusters.
- A cut at ~6.5 distance would result in 3 clusters.-

## Hierarchical Clustering (Final Model)
"""

# Drop non-numeric columns if they exist
non_numeric_cols = ["Ticker Symbol", "Security", "GICS Sector", "GICS Sub Industry"]
df_numeric = df_scaled.drop(columns=non_numeric_cols, errors="ignore")

# Apply Agglomerative Clustering on numeric data
hierarchical_model = AgglomerativeClustering(n_clusters=3, linkage="average", metric="euclidean")
df_scaled["Cluster_Hierarchical"] = hierarchical_model.fit_predict(df_numeric)

# Check the distribution of clusters
print(df_scaled["Cluster_Hierarchical"].value_counts())

from sklearn.decomposition import PCA

# Apply PCA for 2D visualization
pca = PCA(n_components=2)
pca_result = pca.fit_transform(df_numeric)  # Use only numeric columns

df_scaled["PCA1"] = pca_result[:, 0]
df_scaled["PCA2"] = pca_result[:, 1]

# Scatter plot of PCA components
plt.figure(figsize=(8, 6))
sns.scatterplot(x="PCA1", y="PCA2", hue="Cluster_Hierarchical", data=df_scaled, palette="viridis", alpha=0.8)

plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.title("PCA Visualization of Hierarchical Clusters")
plt.legend(title="Cluster")
plt.show()

"""### Summary Statistics for Each Cluster"""

# Compute the mean values of numerical features for each cluster
cluster_means = df_numeric.groupby(df_scaled["Cluster_Hierarchical"]).mean()
print(cluster_means)

"""## Visualizing Cluster Differences"""

from sklearn.decomposition import PCA
import seaborn as sns
import matplotlib.pyplot as plt

# Remove any non-numeric columns (ensure only numerical data is used)
numeric_columns = df_scaled.select_dtypes(include=['number']).columns
df_numeric = df_scaled[numeric_columns]

# Apply PCA to reduce dimensions to 2
pca = PCA(n_components=2)
df_pca = pca.fit_transform(df_numeric)

# Convert PCA results into a DataFrame
df_pca = pd.DataFrame(df_pca, columns=["PC1", "PC2"])
df_pca["Cluster_KMeans"] = df_scaled["Cluster_KMeans"].values
df_pca["Cluster_Hierarchical"] = df_scaled["Cluster_Hierarchical"].values

# Plot K-Means Clusters
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.scatterplot(data=df_pca, x="PC1", y="PC2", hue="Cluster_KMeans", palette="coolwarm", alpha=0.7)
plt.title("K-Means Clustering (PCA Projection)")

# Plot Hierarchical Clusters
plt.subplot(1, 2, 2)
sns.scatterplot(data=df_pca, x="PC1", y="PC2", hue="Cluster_Hierarchical", palette="coolwarm", alpha=0.7)
plt.title("Hierarchical Clustering (PCA Projection)")

plt.tight_layout()
plt.show()

"""# PCA Scatter Plots Analysis

## K-Means Clustering (Left Plot)
- The plot shows **four clusters** (0, 1, 2, 3), even though we expected three.
- Some clusters appear **overlapping**, suggesting that K-Means struggled to achieve clear separations.
- The **red cluster (3)** is distinctly separated, indicating a unique financial profile.

## Hierarchical Clustering (Right Plot)
- The plot correctly forms **three clusters** (0, 1, 2).
- The clusters appear **better separated** than those in K-Means.
- The **red cluster (2)** is isolated, suggesting a well-defined subgroup.
- The **gray cluster (1)** is spread out, possibly representing a diverse set of stocks.

## Inference
- **Hierarchical clustering** provides **better-separated clusters**, making it easier to distinguish groups.
- **K-Means clustering**, while overlapping, ensures **balanced cluster sizes**.
- If the goal is **interpretability and clear separation**, hierarchical clustering performs better.
- If **even distribution across clusters** is needed, K-Means might be preferable.


---

# **Observations**

1. **Cluster Formation**  
   - K-Means formed **4 clusters** with a more even distribution.  
   - Hierarchical clustering also identified **4 clusters**, but with a more imbalanced distribution.  
   - Both clustering methods revealed similar patterns, with slight variations in size and group characteristics.  

2. **Elbow and Silhouette Scores**  
   - The **Elbow method** suggests an optimal number of clusters at **k = 4 or 5**.  
   - The **Silhouette score** is highest around **k = 4**, indicating well-separated clusters.  

3. **Cluster Distributions**  
   - **K-Means Clusters:**  
     - **Cluster 2** (largest, 188 stocks): Represents financially stable companies.  
     - **Cluster 3** (smallest, 38 stocks): High volatility and EPS, suggesting risky high-growth stocks.  
   - **Hierarchical Clusters:**  
     - **Cluster 4** (largest, 221 stocks): Moderate financial indicators, mid-tier firms.  
     - **Cluster 1** (smallest, 25 stocks): High liquidity, strong cash flow companies.  

4. **Financial Characteristics (Heatmap Insights)**  
   - **Cluster 1 (Hierarchical):** High Net Cash Flow and Cash Ratio ‚Üí Strong liquidity.  
   - **Cluster 2 (Hierarchical):** High P/E Ratio and Volatility ‚Üí High-risk, high-reward stocks.  
   - **Cluster 3 (Hierarchical):** High Net Income, ROE, and EPS ‚Üí Profitable, well-established firms.  
   - **Cluster 4 (Hierarchical):** Moderate financial indicators ‚Üí Majority of mid-tier companies.  

5. **Cophenetic Correlation (Clustering Performance)**  
   - **Average Linkage (0.8409)** showed the highest correlation, meaning it best preserves original distances.  
   - **Single Linkage (0.8040)** performed well but is prone to chaining effects.  
   - **Complete Linkage (0.7369) & Weighted Linkage (0.7082)** performed relatively lower.  



 ---

# **Inference**

- **Cluster 2 (K-Means) & Cluster 4 (Hierarchical) represent the largest segment**, consisting of **financially stable, mid-tier companies**.  
- **Smaller clusters (K-Means: Cluster 3, Hierarchical: Cluster 1) highlight high-risk stocks** with high volatility and EPS.  
- **Liquidity and financial health vary significantly across clusters**, which could guide investment strategies.  
- **Hierarchical clustering provides better visual insights via dendrograms**, while K-Means offers a better-spread distribution.  


---

# **Conclusion**

- **K-Means provided a more evenly spread clustering**, making it easier to analyze different financial segments.  
- **Hierarchical clustering yielded imbalanced clusters**, but its dendrogram provided clearer separations.  
- **Both methods suggested 4 clusters**, reinforcing the validity of the grouping.  
- **Financial characteristics align well with the clusters**, making them meaningful for investment analysis.  
- **Cophenetic correlation confirms that "average linkage" was the best clustering approach** in preserving distance relationships.  

---

# **Recommendations**

### **1. For Investors:**
- **Stable investment?** Focus on **Cluster 2 (K-Means) / Cluster 4 (Hierarchical)**.  
- **Growth stocks?** Look at **Cluster 1 (K-Means) / Cluster 2 (Hierarchical)** for high P/E ratios.  
- **Low-risk stocks?** Consider **Cluster 1 (Hierarchical)** for strong liquidity.  
- **Risky but high-reward?** Investigate **Cluster 3 (K-Means) / Cluster 3 (Hierarchical)** for high EPS and volatility.  


### **2. For Business Decision-Making:**
- Use clustering insights to segment **investment portfolios** based on risk tolerance.  
- Target financial strategies based on **cluster-specific characteristics** (e.g., risk appetite, cash flow strength).  
- Adjust business models to match industry **trends identified in each cluster**.  

---

## **Final project statement :**
This project successfully clusters stocks into meaningful investment groups, allowing investors to make data-driven decisions. Hierarchical Clustering performed better than K-Means, but both methods offer useful insights into stock classification.
"""